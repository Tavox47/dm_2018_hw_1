{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: Gustavo Cespedes\n",
    "\n",
    "Student ID: 106077446\n",
    "\n",
    "GitHub ID: Tavox47"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, you should attempt the **take home** exercises provided in the [notebook](https://github.com/omarsar/data_mining_lab/blob/master/news_data_mining.ipynb) we used for the first lab session. Attempt all the exercises, as it is counts towards the final grade of your first assignment (20%). \n",
    "\n",
    "- Then, download the dataset provided in this [link](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#). The sentiment dataset contains a `sentence` and `score` label. Read the specificiations of the dataset before you start exploring it. \n",
    "\n",
    "\n",
    "- Then, you are asked to apply each of the data exploration and data operation steps learned in the [first lab session](https://github.com/omarsar/data_mining_lab) on **the new dataset**. You don't need to explain all the procedures as we did in the notebook, but you are expected to provide some **minimal comments** explaining your code. You are also expected to use the same libraries used in the first lab session. You are allowed to use and modify the `helper` functions we provided in the first lab session or create your own. Also, be aware that the helper functions may need modification as you are dealing with a completely different dataset. This part is worth 30% of your grade!\n",
    "\n",
    "- In addition to applying the same operations from the first lab, we are asking that you attempt the following tasks on the new sentiment dataset as well (40%):\n",
    "    - Use your creativity and imagination to generate **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n",
    "    - Generate **TF-IDF features** from the tokens of each text. Refer to this Sciki-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) on how you may go about doing this. Keep in mind that you are generating a matrix similar to the term-document matrix we implemented in our first lab session. However, the weights will be computed differently and should represent the TF-IDF value of each word per document as opposed to the word frequency.\n",
    "    - Using both the TF-IDF and word frequency features, try to compute the **similarity** between random sentences and report results. Read the \"distance simiilarity\" section of the Data Mining textbook on what measures you can use here. [Cosine similarity](https://jamesmccaffrey.wordpress.com/2017/03/29/the-cosine-similarity-of-two-sentences/) is one of these methods but there are others. Try to explore a few of them in this exercise and report the differences in result. \n",
    "    - Lastly, implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Try to implement this using scikit-learn built in classifiers and use both the TF-IDF features and word frequency features to build two seperate classifiers. Refer to this [nice article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/) on how to build this type of classifier using scikit-learn. Report the classification accuracy of both your models. If you are struggling with this step please reach us on Slack as soon as possible.   \n",
    "\n",
    "\n",
    "- Presentation matters! You are also expected to **tidy up your notebook** and attempt new data operations and techniques that you have learned so far in the Data Mining course. Surprise us! This segment is worth 10% of your grade. The idea of this exercise is to begin thinking of how you will program the concepts you have learned and the process that is involved. \n",
    "\n",
    "\n",
    "- After completing all the above tasks, you are free to remove this header block and **submit** your assignment following the guide provided in the [README.md](https://github.com/omarsar/dm_2018_hw_1/blob/master/README.md) file of the assignment's repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "### Begin Assignment Here!\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array=[]\n",
    "\n",
    "with open(\"amazon_cells_labelled.txt\",'r') as amazon_file:\n",
    "        for sentence in amazon_file.read().split('\\n'):\n",
    "            data_array += [sentence]\n",
    "with open(\"imdb_labelled.txt\",'r') as imdb_file:\n",
    "        for sentence in imdb_file.read().split('\\n'):\n",
    "            data_array += [sentence]\n",
    "with open(\"yelp_labelled.txt\",'r') as yelp_file:\n",
    "        for sentence in yelp_file.read().split('\\n'):\n",
    "            data_array += [sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               sentence number\n",
      "0     So there is no way for me to plug it in here i...      0\n",
      "1                           Good case, Excellent value.      1\n",
      "2                                Great for the jawbone.      1\n",
      "3     Tied to charger for conversations lasting more...      0\n",
      "4                                     The mic is great.      1\n",
      "5     I have to jiggle the plug to get it to line up...      0\n",
      "6     If you have several dozen or several hundred c...      0\n",
      "7           If you are Razr owner...you must have this!      1\n",
      "8                   Needless to say, I wasted my money.      0\n",
      "9                      What a waste of money and time!.      0\n",
      "10                      And the sound quality is great.      1\n",
      "11    He was very impressed when going from the orig...      1\n",
      "12    If the two were seperated by a mere 5+ ft I st...      0\n",
      "13                             Very good quality though      1\n",
      "14    The design is very odd, as the ear \"clip\" is n...      0\n",
      "15    Highly recommend for any one who has a blue to...      1\n",
      "16                  I advise EVERYONE DO NOT BE FOOLED!      0\n",
      "17                                     So Far So Good!.      1\n",
      "18                                        Works great!.      1\n",
      "19    It clicks into place in a way that makes you w...      0\n",
      "20    I went on Motorola's website and followed all ...      0\n",
      "21    I bought this to use with my Kindle Fire and a...      1\n",
      "22             The commercials are the most misleading.      0\n",
      "23    I have yet to run this new battery below two b...      1\n",
      "24    I bought it for my mother and she had a proble...      0\n",
      "25                 Great Pocket PC / phone combination.      1\n",
      "26    I've owned this phone for 7 months now and can...      1\n",
      "27    I didn't think that the instructions provided ...      0\n",
      "28    People couldnt hear me talk and I had to pull ...      0\n",
      "29                                 Doesn't hold charge.      0\n",
      "...                                                 ...    ...\n",
      "2972                    The ambiance isn't much better.      0\n",
      "2973  Unfortunately, it only set us up for disapppoi...      0\n",
      "2974                              The food wasn't good.      0\n",
      "2975  Your servers suck, wait, correction, our serve...      0\n",
      "2976      What happened next was pretty....off putting.      0\n",
      "2977  too bad cause I know it's family owned, I real...      0\n",
      "2978               Overpriced for what you are getting.      0\n",
      "2979               I vomited in the bathroom mid lunch.      0\n",
      "2980  I kept looking at the time and it had soon bec...      0\n",
      "2981  I have been to very few places to eat that und...      0\n",
      "2982  We started with the tuna sashimi which was bro...      0\n",
      "2983                            Food was below average.      0\n",
      "2984  It sure does beat the nachos at the movies but...      0\n",
      "2985       All in all, Ha Long Bay was a bit of a flop.      0\n",
      "2986  The problem I have is that they charge $11.99 ...      0\n",
      "2987  Shrimp- When I unwrapped it (I live only 1/2 a...      0\n",
      "2988     It lacked flavor, seemed undercooked, and dry.      0\n",
      "2989  It really is impressive that the place hasn't ...      0\n",
      "2990  I would avoid this place if you are staying in...      0\n",
      "2991  The refried beans that came with my meal were ...      0\n",
      "2992         Spend your money and time some place else.      0\n",
      "2993  A lady at the table next to us found a live gr...      0\n",
      "2994            the presentation of the food was awful.      0\n",
      "2995           I can't tell you how disappointed I was.      0\n",
      "2996  I think food should have flavor and texture an...      0\n",
      "2997                           Appetite instantly gone.      0\n",
      "2998  Overall I was not impressed and would not go b...      0\n",
      "2999  The whole experience was underwhelming, and I ...      0\n",
      "3000  Then, as if I hadn't wasted enough of my life ...      0\n",
      "3001                                                          \n",
      "\n",
      "[3002 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "sentence_array=[]\n",
    "number_array=[]\n",
    "\n",
    "for sentence in data_array:\n",
    "    temp_sentence = sentence.split('\\t')\n",
    "    sentence_array += [temp_sentence[0]]\n",
    "    number_array += [temp_sentence[-1]]\n",
    "\n",
    "dataframe={'sentence':sentence_array,'number':number_array}\n",
    "dataset= pd.DataFrame(data=dataframe)\n",
    "\n",
    "print (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
